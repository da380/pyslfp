{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47264ff7",
   "metadata": {},
   "source": [
    "# Tutorial 5: A Bayesian Inverse Problem - Inferring Ice Melt from Tide Gauges\n",
    "\n",
    "In our previous tutorials, we have focused on the **forward problem**: predicting the Earth's response to a *known* change in ice mass. In this tutorial, we will tackle the more challenging **inverse problem**: inferring an *unknown* ice mass change from a set of sparse and noisy observations.\n",
    "\n",
    "We will use the full power of the `pyslfp` and `pygeoinf` libraries to solve this problem within a **Bayesian framework**. Instead of finding a single \"best-fit\" solution, the Bayesian method provides a full **posterior probability distribution**. From this, we can extract the **posterior expectation**, which represents our most likely estimate of the ice melt pattern. The full posterior distribution also characterises the uncertainty in our knowledge about the ice mass change. Methods exist within `pygeoinf`  for working practically with the full posterior distribution though their computational cost remains rather high. Here, we will instead use the posterior distribution to fully characterise our knowledge of the global mean sea level (GMSL) change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d8ca5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by importing the necessary components from `pyslfp`, `pygeoinf`, and other standard libraries. We will also initialize our `FingerPrint` model, which provides the underlying physics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pyslfp as sl\n",
    "import pygeoinf as inf\n",
    "import cartopy.crs as ccrs\n",
    "import scipy.stats as stats\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# Initialize the core fingerprint model - lower lmax to reduce calculation times for this tutorial.\n",
    "fp = sl.FingerPrint(\n",
    "    lmax=256,\n",
    "    earth_model_parameters=sl.EarthModelParameters.from_standard_non_dimensionalisation(),\n",
    ")\n",
    "fp.set_state_from_ice_ng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705420f",
   "metadata": {},
   "source": [
    "## Step 1: Loading the Tide Gauge Data\n",
    "\n",
    "We will use the real locations from the GLOSS tide gauge network. The `read_gloss_tide_gauge_data` function in our `utils` module handles loading this data for us.\n",
    "\n",
    "All of the listed stations can be used, or a random sub-sample can be chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcdae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full list of GLOSS tide gauge stations\n",
    "lats, lons = sl.read_gloss_tide_gauge_data()\n",
    "\n",
    "# --- Configuration for data selection ---\n",
    "use_all_stations = True\n",
    "if not use_all_stations:\n",
    "    number_of_stations_to_sample = 100\n",
    "    \n",
    "\n",
    "# -----------------------------------------\n",
    "if use_all_stations:\n",
    "    tide_gauge_points = list(zip(lats, lons))\n",
    "else:\n",
    "    # Create a reproducible random sample of stations\n",
    "    random.seed(123)\n",
    "    indices = random.sample(range(len(lats)), number_of_stations_to_sample)\n",
    "    sampled_lats = [lats[i] for i in indices]\n",
    "    sampled_lons = [lons[i] for i in indices]\n",
    "    tide_gauge_points = list(zip(sampled_lats, sampled_lons))\n",
    "\n",
    "print(f\"Using {len(tide_gauge_points)} tide gauge stations for the inversion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a42ad",
   "metadata": {},
   "source": [
    "## Step 2: Defining the Model Space\n",
    "\n",
    "The next step is to define the space of possible solutions. Our unknown is the global pattern of **ice thickness change**. Within this application, we suppose that changes in ice thickness can only occur where there is already ice within our background model. To achieve this, we will start with a defined over the whole sphere, and then mutliply by a masking function that is one over the ice sheets and zero elsewhere. \n",
    "\n",
    "For the underlying space of functions, we use a `pygeoinf.symmetric_space.sphere.Sobolev` space with a prescribed order and smoothness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591df7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model space for the unknown ice thickness change\n",
    "order = 2.0\n",
    "scale_km = 250.0\n",
    "scale = scale_km * 1000 / fp.length_scale\n",
    "\n",
    "model_space = inf.symmetric_space.sphere.Sobolev(\n",
    "    fp.lmax, order, scale, radius=fp.mean_sea_floor_radius\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ac2c7",
   "metadata": {},
   "source": [
    "## Step 3: Defining the Forward Operator\n",
    "\n",
    "Next, we must define the **forward operator**. This is the mathematical object that represents the complete chain of physical processes and measurements linking our model (ice thickness change) to our data (tide gauge readings).\n",
    "\n",
    "Using the operator algebra of `pygeoinf`, we can build this complex chain by composing several simpler operators:\n",
    "\n",
    "1.  **`ice_projection_operator`**: Takes a scalar-field defined everywhere on the surface and multiplied by a function equal to one over the current ice sheets. \n",
    "2.  **`ice_thickness_change_to_load_operator`**: Takes an ice thickness change field and converts it to a surface mass load.\n",
    "3.  **`fp.as_sobolev_linear_operator`**: The main fingerprint operator. It takes the mass load and calculates the full physical response: `[sea-level, displacement, gravity, rotation]`.\n",
    "4.  **`tide_gauge_operator`**: This takes the full response and evaluates the sea level change at the discrete locations of our tide gauges.\n",
    "\n",
    "The final forward operator, `A`, is the composition of all three steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f78b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps scalar-fields on sphere to be non-zero only over background ice sheets. \n",
    "op1 = sl.ice_projection_operator(fp, model_space)\n",
    "\n",
    "# Maps an ice thickness change to the corresponding load. \n",
    "op2 = sl.ice_thickness_change_to_load_operator(fp, model_space)\n",
    "\n",
    "# Maps a direct load to the full response. \n",
    "op3 = fp.as_sobolev_linear_operator(order, scale, rtol=1e-9)\n",
    "\n",
    "# Maps the full response to the tide gauge values. \n",
    "op4 = sl.operators.tide_gauge_operator(op3.codomain, tide_gauge_points)\n",
    "\n",
    "# Form the forward operator by composition. \n",
    "A = op4@ op3 @ op2 @ op1\n",
    "data_space = A.codomain\n",
    "\n",
    "# Form also a mapping from the model space to the sea level field for convenience. \n",
    "P = op3.codomain.subspace_projection(0)\n",
    "A_sl = P @ op3 @ op2 @ op1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1dc16",
   "metadata": {},
   "source": [
    "## Step 4: Setting up the LinearForwardProblem\n",
    "\n",
    "Now we bundle the forward operator `A` with a model for our measurement errors. We'll assume the tide gauge measurements are corrupted by random, uncorrelated Gaussian noise with a given standard deviation. The amplitude of the errors is chosen to be commensurate with the synethetic sea level changes defined below. \n",
    "\n",
    "The `pygeoinf.LinearForwardProblem` class encapsulates this entire structure: **data = A(model) + error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c44f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data error statistics\n",
    "tide_gauge_std_dev_m = 0.001\n",
    "tide_gauge_std_dev = tide_gauge_std_dev_m / fp.length_scale\n",
    "data_error_measure = inf.GaussianMeasure.from_standard_deviation(\n",
    "    data_space, tide_gauge_std_dev\n",
    ")\n",
    "\n",
    "# Bundle everything into a forward problem object\n",
    "forward_problem = inf.LinearForwardProblem(\n",
    "    A, data_error_measure=data_error_measure\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1a829",
   "metadata": {},
   "source": [
    "## Step 5: Creating a synthetic data set\n",
    "\n",
    "To test our inversion, we need synthetic data set generated from a chosen model.\n",
    "\n",
    "We do this by defining a **prior probability distribution** on the model space. This \"prior\" represents our beliefs about the ice thickness change *before* seeing any data. \n",
    "\n",
    "For our prior, we start with a standard rotationally invariant Gaussian measure whose typical pointwise amplitude has been set to a desired value. We can then push this measure forward under the operator that takes functions over the whose surface to those that are non-zero only over the background ice sheets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial model prior measure\n",
    "pointwise_std_m = 0.1\n",
    "pointwise_std = pointwise_std_m / fp.length_scale\n",
    "initial_model_prior_measure = model_space.point_value_scaled_heat_kernel_gaussian_measure(scale, pointwise_std)\n",
    "\n",
    "# Transform so that ice thickness change non-zero only over current ice sheets. \n",
    "model_prior_measure = initial_model_prior_measure.affine_mapping(operator=op1)\n",
    "\n",
    "# --- Generate the synthetic ground truth and noisy data ---\n",
    "model_true, data = forward_problem.synthetic_model_and_data(model_prior_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a606f",
   "metadata": {},
   "source": [
    "## Step 6: Solving the Bayesian Inverse Problem\n",
    "\n",
    "Now we have all the ingredients to solve the inverse problem. We use the `pygeoinf.LinearBayesianInversion` class, providing it with our forward problem and our prior.\n",
    "\n",
    "The `model_posterior_measure` method applies Bayes' theorem to combine the prior information with the information from the data. The result is the **posterior probability distribution**, which represents our updated, combined state of knowledge.\n",
    "\n",
    "In solving the Bayesian inverse problem it is necessary to repeatedly solve a linear system of equations on the data-space known as the Bayesian normal equations. The relevant linear operator can be written\n",
    "$$\n",
    "N = AQA^{*} + R, \n",
    "$$\n",
    "where $A$ is the forward operator, $Q$ the model covariance, and $R$ the data covariance. As can be seen, each action of this operator requires one forward and one adjoint action of $A$, with in this case each operation involving a solution of the sea level equation. \n",
    "\n",
    "There are several methods available for solving this linear system, with `pygeoinf` requiring the user to select a solution method either from a standard set or by implementing their own custom approach. For problems with either large data sets or expensive forward problems it is generally best to apply iterative methods, and that is what is done here. Iterative methods, however, benifit massively from a good preconditioner being supplied, this being an operator that approximately solves the linear system, but whose action is relatively cheap to compute. \n",
    "\n",
    "To form a preconditioner in this case we will re-define our forward operator and model prior but using a substantially lower truncation degree in the calculations. Doing so lowers the accuracy of their results, but is also lowers the calcuation time dramatically. Having done this, we can form the associated Bayesian inverse problem at the lower truncation degree and form its normal operator which is, of course, define on the same data space as our initial problem. The matrix form of this new normal operator can be computed explicitly at reasonable cost, and the action of its inverse determine cheaply by Cholesky factorising it and then performing back substitution for each new right hand side. In this manner we arrive at a reasonable preconditioner for the problem at hand. \n",
    "\n",
    "In the code block below we reform the necessary forward problem and prior at the lower degree. In practice, it would have been useful to wrap this construction process within a function to avoid code repetition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59969d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the set up of the forward problem, but now using a lower lmax\n",
    "lmax_precon = 32\n",
    "\n",
    "fp_precon = sl.FingerPrint(\n",
    "    lmax=lmax_precon,\n",
    "    earth_model_parameters=sl.EarthModelParameters.from_standard_non_dimensionalisation(),\n",
    ")\n",
    "fp_precon.set_state_from_ice_ng()\n",
    "\n",
    "model_space_precon = inf.symmetric_space.sphere.Sobolev(\n",
    "    fp_precon.lmax, order, scale, radius=fp_precon.mean_sea_floor_radius\n",
    ")\n",
    "\n",
    "op1_precon = sl.ice_projection_operator(fp_precon, model_space_precon)\n",
    "op2_precon = sl.ice_thickness_change_to_load_operator(fp_precon, model_space_precon)\n",
    "op3_precon = fp_precon.as_sobolev_linear_operator(order, scale, rtol=1e-9)\n",
    "op4_precon = sl.operators.tide_gauge_operator(op3_precon.codomain, tide_gauge_points)\n",
    "A_precon = op4_precon@ op3_precon @ op2_precon @ op1_precon\n",
    "\n",
    "initial_model_prior_measure_precon = model_space_precon.point_value_scaled_heat_kernel_gaussian_measure(scale, pointwise_std)\n",
    "model_prior_measure_precon = initial_model_prior_measure_precon.affine_mapping(operator=op1_precon)\n",
    "\n",
    "forward_problem_precon = inf.LinearForwardProblem(\n",
    "    A_precon, data_error_measure=data_error_measure\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28d049",
   "metadata": {},
   "source": [
    "Having formed the reduced-degree problem, we can set up the necessary Bayesian inverse problems, extract the approximate normal operator for preconditioning, and then solve the full problem iteratively. Note that running this calculation at degree 256 will still take several minutes, but without preconditioning this would be substantially greater. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882518c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the inversion for the preconditioning system\n",
    "bayesian_inversion_precon = inf.LinearBayesianInversion(forward_problem_precon, model_prior_measure_precon)\n",
    "\n",
    "# Get the normal operator for the preconditioning system.\n",
    "normal_operator_precon = bayesian_inversion_precon.normal_operator\n",
    "\n",
    "# Form its inverse using Eigen-decomposition. \n",
    "print(\"Forming the preconditioner...\")\n",
    "solver = inf.EigenSolver(parallel=True)\n",
    "inverse_normal_operator_precon = solver(normal_operator_precon)\n",
    "\n",
    "# Set up the Bayesian inversion method\n",
    "bayesian_inversion = inf.LinearBayesianInversion(forward_problem, model_prior_measure)\n",
    "\n",
    "# Solve for the posterior distribution\n",
    "print(\"Solving the linear system...\")\n",
    "model_posterior_measure = bayesian_inversion.model_posterior_measure(\n",
    "    data, inf.CGMatrixSolver(), preconditioner= inverse_normal_operator_precon\n",
    ")\n",
    "\n",
    "# Get the posterior expectation\n",
    "model_posterior_expectation = model_posterior_measure.expectation\n",
    "\n",
    "# print out the total number of calls to the fingerpint solver - one will be for generating the synthetic data. \n",
    "print(f'Number of solutions of the fingerprint problem = {fp.solver_counter}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff9204",
   "metadata": {},
   "source": [
    "## Step 7: Analyzing and Visualizing the Result\n",
    "\n",
    "The primary result we are interested in is the **posterior expectation**. This is our best estimate of the true ice thickness change, given our prior beliefs and the information contained in the tide gauge data.\n",
    "\n",
    "The full posterior distribution also contains the **posterior covariance**, which quantifies our uncertainty. For this tutorial, we will focus solely on the posterior expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57073381",
   "metadata": {},
   "source": [
    "### Plotting the Inferred Ice Melt\n",
    "\n",
    "First, we will create two separate plots to compare the \"ground truth\" ice melt with our inferred result:\n",
    "1.  **The True Model:** The actual ice thickness change we were trying to recover.\n",
    "2.  **The Posterior Expectation:** Our best estimate, which we hope resembles the true model.\n",
    "\n",
    "To ensure a fair visual comparison, we will calculate a common color scale based on the maximum absolute value across both fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate a shared, symmetric color scale for the ice thickness plots ---\n",
    "max_abs_ice_change = np.nanmax(\n",
    "    np.abs(\n",
    "        np.concatenate([\n",
    "            (model_true).data.flatten(),\n",
    "            (model_posterior_expectation).data.flatten()\n",
    "        ])\n",
    "    )\n",
    ") * 1000 * fp.length_scale\n",
    "\n",
    "# --- Plot 1: The \"Ground Truth\" Model ---\n",
    "fig1, ax1, im1 = sl.plot(\n",
    "    1000* model_true * fp.length_scale,\n",
    "    coasts=True,\n",
    "    cmap=\"seismic\",\n",
    "    vmin=-max_abs_ice_change,\n",
    "    vmax=max_abs_ice_change\n",
    ")\n",
    "ax1.set_title(\"a) True Ice Thickness Change\")\n",
    "fig1.colorbar(im1, ax=ax1, orientation=\"horizontal\", pad=0.05, shrink=0.7, label=\"Ice Thickness Change (mm)\")\n",
    "\n",
    "\n",
    "# --- Plot 2: The Posterior Expectation (Our Best Estimate) ---\n",
    "fig2, ax2, im2 = sl.plot(\n",
    "    1000* model_posterior_expectation  * fp.length_scale,\n",
    "    coasts=True,\n",
    "    cmap=\"seismic\",\n",
    "    vmin=-max_abs_ice_change,\n",
    "    vmax=max_abs_ice_change\n",
    ")\n",
    "ax2.set_title(\"b) Posterior Expectation (Inferred from Data)\")\n",
    "\n",
    "# Mark the tide gauge locations\n",
    "lats = [p[0] for p in tide_gauge_points]\n",
    "lons = [p[1] for p in tide_gauge_points]\n",
    "#ax2.plot(lons, lats, 'm^', markersize=5, transform=ccrs.PlateCarree())\n",
    "\n",
    "fig2.colorbar(im2, ax=ax2, orientation=\"horizontal\", pad=0.05, shrink=0.7, label=\"Ice Thickness Change (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c382496",
   "metadata": {},
   "source": [
    "### Plotting the Predicted Sea Level\n",
    "\n",
    "As a final check, we can use our inferred ice melt pattern (`model_posterior_expectation`) to predict the global sea-level fingerprint. By comparing this predicted fingerprint to the \"true\" fingerprint, we can visually assess how well our inversion has captured the large-scale signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf64bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the predicted sea-level field from the posterior expectation model\n",
    "sea_level_posterior = A_sl(model_posterior_expectation)\n",
    "\n",
    "# Calculate the true sea level change. \n",
    "sea_level_true = A_sl(model_true)\n",
    "\n",
    "# --- Calculate a shared, symmetric color scale for the sea-level plots ---\n",
    "ocean_mask = fp.ocean_projection()\n",
    "max_abs_sl_change = np.nanmax(\n",
    "    np.abs(\n",
    "        np.concatenate([\n",
    "            (sea_level_true * ocean_mask).data.flatten(),\n",
    "            (sea_level_posterior * ocean_mask).data.flatten()\n",
    "        ])\n",
    "    )\n",
    ") * 1000* fp.length_scale\n",
    "\n",
    "\n",
    "# --- Plot 3: The \"True\" Sea-Level Field ---\n",
    "fig1, ax1, im1 = sl.plot(\n",
    "    1000 * sea_level_true * ocean_mask * fp.length_scale,    \n",
    "    coasts=True,\n",
    "    cmap=\"seismic\",\n",
    "    vmin=-max_abs_sl_change,\n",
    "    vmax=max_abs_sl_change\n",
    ")\n",
    "ax1.set_title(\"a) True Sea-Level Fingerprint\")\n",
    "ax1.plot(lons, lats, 'm^', markersize=5, transform=ccrs.PlateCarree())\n",
    "fig1.colorbar(im1, ax=ax1, orientation=\"horizontal\", pad=0.05, shrink=0.7, label=\"Sea Level Change (mm)\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Plot 4: The Sea-Level Field Predicted by the Inversion ---\n",
    "fig2, ax2, im2 = sl.plot(\n",
    "    1000 * sea_level_posterior * fp.ocean_projection()* fp.length_scale,\n",
    "    coasts=True,\n",
    "    cmap=\"seismic\",\n",
    "    vmin=-max_abs_sl_change,\n",
    "    vmax=max_abs_sl_change\n",
    ")\n",
    "ax2.set_title(\"b) Predicted Sea-Level Fingerprint\")\n",
    "ax2.plot(lons, lats, 'm^', markersize=5, transform=ccrs.PlateCarree())\n",
    "fig2.colorbar(im2, ax=ax2, orientation=\"horizontal\", pad=0.05, shrink=0.7, label=\"Sea Level Change (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef038ea",
   "metadata": {},
   "source": [
    "## Mapping the posterior onto a lower-dimensional space\n",
    "\n",
    "Comparison of the true and posterior mean ice thickness changes suggests that while the tide gauge data can recover broadly correct features of the ice thickness change, it is not possible to recover fine spatial details. This is not surprising given the relative sparsity of the data and the fact that most tide gauges are located far from the polar regions. \n",
    "\n",
    "Despite this limitation, we can still investigate the extent to which certain average properties of the ice thickness change can be determined. First, we look at the estimated global mean sea level (GMSL) change. To do so, we set up, via a simple mass balance, a LinearOperator from the model space to resulting 1D property space. The model posterior measure can be pushed forward under this mapping, and we can then visualise the resulting distribution. \n",
    "\n",
    "Note that pushing forward the posterior measure to an n-dimensional space on which we determine the concrete form of the resulting covariance requires n additional solutions of the Bayesian normal equations. As a result, the calculation here will take a comparable time to that above for finding the full posterior expectation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7259b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the weighting function for GMSL estimates  - Note that length scale factor to dimensionalise the result into mm\n",
    "GMSL_weighting_function =  -fp.ice_density * fp.one_minus_ocean_function * fp.ice_projection(value=0) * 1000 * fp.length_scale / (fp.water_density * fp.ocean_area)\n",
    "\n",
    "# Form the mapping to GSML. \n",
    "B = sl.averaging_operator(model_space, [GMSL_weighting_function])\n",
    "\n",
    "# Get the true GMSL\n",
    "GMSL_true = B(model_true)\n",
    "\n",
    "# Push forward the posterior to the GMSL space.\n",
    "GMSL_prior_measure = model_prior_measure.affine_mapping(operator=B)\n",
    "GMSL_posterior_measure = model_posterior_measure.affine_mapping(operator=B)\n",
    "\n",
    "\n",
    "# 1. Get statistics for the POSTERIOR distribution\n",
    "gmsl_posterior_mean = GMSL_posterior_measure.expectation[0]\n",
    "gmsl_posterior_var = GMSL_posterior_measure.covariance.matrix(dense=True)[0, 0]\n",
    "gmsl_posterior_std = np.sqrt(gmsl_posterior_var)\n",
    "\n",
    "# 2. Get statistics for the PRIOR distribution\n",
    "gmsl_prior_mean = GMSL_prior_measure.expectation[0]\n",
    "gmsl_prior_var = GMSL_prior_measure.covariance.matrix(dense=True)[0, 0]\n",
    "gmsl_prior_std = np.sqrt(gmsl_prior_var)\n",
    "\n",
    "\n",
    "# 2. Define an x-axis that covers both distributions\n",
    "x_min = min(gmsl_prior_mean - 6 * gmsl_prior_std, gmsl_posterior_mean - 6 * gmsl_posterior_std)\n",
    "x_max = max(gmsl_prior_mean + 6 * gmsl_prior_std, gmsl_posterior_mean + 6 * gmsl_posterior_std)\n",
    "x_axis = np.linspace(x_min, x_max, 1000)\n",
    "\n",
    "# 3. Calculate the PDF values manually using the mean and std\n",
    "prior_pdf_values = stats.norm.pdf(x_axis, loc=gmsl_prior_mean, scale=gmsl_prior_std)\n",
    "posterior_pdf_values = stats.norm.pdf(x_axis, loc=gmsl_posterior_mean, scale=gmsl_posterior_std)\n",
    "\n",
    "# 4. Create the plot with two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot the PRIOR on the first axis (ax1)\n",
    "color1 = 'green'\n",
    "ax1.set_xlabel('Total GMSL Change (mm)')\n",
    "ax1.set_ylabel('Prior Probability Density', color=color1)\n",
    "ax1.plot(x_axis, prior_pdf_values, color=color1, lw=2, linestyle=':', label='Prior PDF')\n",
    "ax1.fill_between(x_axis, prior_pdf_values, color=color1, alpha=0.15)\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.grid(True, linestyle='--')\n",
    "\n",
    "# Create a second y-axis that shares the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the POSTERIOR on the second axis (ax2)\n",
    "color2 = 'blue'\n",
    "ax2.set_ylabel('Posterior Probability Density', color=color2)\n",
    "ax2.plot(x_axis, posterior_pdf_values, color=color2, lw=2, label='Posterior PDF (Estimated)')\n",
    "ax2.fill_between(x_axis, posterior_pdf_values, color=color2, alpha=0.2)\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "ax2.grid(False)\n",
    "\n",
    "# 5. Plot the vertical lines for reference (plotted on the first axis)\n",
    "line1 = ax1.axvline(gmsl_posterior_mean, color='red', linestyle='--', lw=2, label=f'Posterior Mean: {gmsl_posterior_mean:.5f} mm')\n",
    "line2 = ax1.axvline(GMSL_true[0], color='black', linestyle='-', lw=2, label=f'True Value: {GMSL_true[0]:.5f} mm')\n",
    "\n",
    "\n",
    "# 6. Create a single, combined legend\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "all_handles = handles + handles2\n",
    "all_labels = [h.get_label() for h in all_handles]\n",
    "\n",
    "fig.legend(all_handles, all_labels, loc='upper right', bbox_to_anchor=(0.9, 0.9))\n",
    "fig.suptitle('Prior and Posterior Probability Distributions of GMSL', fontsize=16)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654ba39",
   "metadata": {},
   "source": [
    "In a very similar manner, we can estimate a joint measure for the contributions to GMSL change from the Greenland, West Antarctic and East Antarctic ice sheets, with the result displayed using a corner plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot 6: Corner Plot for GMSL Contributions (mm) ---\n",
    "GLI_weighting_function = (\n",
    "    -fp.ice_density\n",
    "    * fp.one_minus_ocean_function\n",
    "    * fp.greenland_projection(value=0)\n",
    "    * 1000* fp.length_scale\n",
    "    / (fp.water_density * fp.ocean_area)\n",
    ")\n",
    "WAI_weighting_function = (\n",
    "    -fp.ice_density\n",
    "    * fp.one_minus_ocean_function\n",
    "    * fp.west_antarctic_projection(value=0)\n",
    "    * 1000* fp.length_scale\n",
    "    / (fp.water_density * fp.ocean_area)\n",
    ")\n",
    "EAI_weighting_function = (\n",
    "    -fp.ice_density\n",
    "    * fp.one_minus_ocean_function\n",
    "    * fp.east_antarctic_projection(value=0)\n",
    "    * 1000 * fp.length_scale\n",
    "    / (fp.water_density * fp.ocean_area)\n",
    ")\n",
    "\n",
    "C = sl.averaging_operator(\n",
    "    model_space,\n",
    "    [GLI_weighting_function, WAI_weighting_function, EAI_weighting_function],\n",
    ")\n",
    "\n",
    "property_true = C(model_true)\n",
    "property_posterior_measure = model_posterior_measure.affine_mapping(operator=C)\n",
    "\n",
    "\n",
    "property_true = C(model_true)\n",
    "property_posterior_measure = model_posterior_measure.affine_mapping(operator=C)\n",
    "\n",
    "# Extract stats (now directly in mm and mm^2)\n",
    "labels = [\"Greenland (mm)\", \"West Antarctica (mm)\", \"East Antarctica (mm)\"]\n",
    "mean_posterior = property_posterior_measure.expectation\n",
    "cov_posterior = property_posterior_measure.covariance.matrix(dense=True, parallel=True)\n",
    "true_values = property_true\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "fig.suptitle(\"Joint Posterior Distribution of GMSL Contributions\", fontsize=16)\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        if i == j:  # Diagonal plots (1D)\n",
    "            mu = mean_posterior[i]\n",
    "            sigma = np.sqrt(cov_posterior[i, i])\n",
    "            true_val = true_values[i]\n",
    "            \n",
    "            x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 200)\n",
    "            pdf = stats.norm.pdf(x, mu, sigma)\n",
    "            ax.plot(x, pdf, \"darkblue\", label=\"Posterior PDF\")\n",
    "            ax.fill_between(x, pdf, color=\"lightblue\", alpha=0.6)\n",
    "            ax.axvline(mu, color=\"red\", linestyle=\"--\", label=f\"Mean: {mu:.2f} mm\")\n",
    "            ax.axvline(\n",
    "                true_val,\n",
    "                color=\"black\",\n",
    "                linestyle=\"-\",\n",
    "                label=f\"True: {true_val:.2f} mm\",\n",
    "            )\n",
    "            ax.set_xlabel(labels[i])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_ylabel(\"Density\" if i == 0 else \"\")\n",
    "\n",
    "        elif i > j:  # Off-diagonal plots (2D)\n",
    "            mean_2d = np.array([mean_posterior[j], mean_posterior[i]])\n",
    "            cov_2d = np.array(\n",
    "                [\n",
    "                    [cov_posterior[j, j], cov_posterior[j, i]],\n",
    "                    [cov_posterior[i, j], cov_posterior[i, i]],\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            x_range = np.linspace(\n",
    "                mean_2d[0] - 4 * np.sqrt(cov_2d[0, 0]),\n",
    "                mean_2d[0] + 4 * np.sqrt(cov_2d[0, 0]),\n",
    "                100,\n",
    "            )\n",
    "            y_range = np.linspace(\n",
    "                mean_2d[1] - 4 * np.sqrt(cov_2d[1, 1]),\n",
    "                mean_2d[1] + 4 * np.sqrt(cov_2d[1, 1]),\n",
    "                100,\n",
    "            )\n",
    "            X, Y = np.meshgrid(x_range, y_range)\n",
    "            pos = np.dstack((X, Y))\n",
    "            rv = stats.multivariate_normal(mean_2d, cov_2d)\n",
    "            Z = rv.pdf(pos)\n",
    "            pcm = ax.pcolormesh(\n",
    "                X,\n",
    "                Y,\n",
    "                Z,\n",
    "                shading=\"auto\",\n",
    "                cmap=\"Blues\",\n",
    "                norm=colors.LogNorm(vmin=Z.min(), vmax=Z.max()),\n",
    "            )\n",
    "            ax.contour(X, Y, Z, colors=\"black\", linewidths=0.5, alpha=0.6)\n",
    "            \n",
    "            # Plot points using direct mm values\n",
    "            ax.plot(\n",
    "                mean_posterior[j],\n",
    "                mean_posterior[i],\n",
    "                \"r+\",\n",
    "                markersize=10,\n",
    "                mew=2,\n",
    "                label=\"Posterior Mean\",\n",
    "            )\n",
    "            ax.plot(\n",
    "                true_values[j],\n",
    "                true_values[i],\n",
    "                \"kx\",\n",
    "                markersize=10,\n",
    "                mew=2,\n",
    "                label=\"True Value\",\n",
    "            )\n",
    "            ax.set_xlabel(labels[j])\n",
    "            ax.set_ylabel(labels[i])\n",
    "\n",
    "        else:  # Hide upper triangle\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "# Final adjustments for corner plot legend\n",
    "handles, labels_leg = axes[0, 0].get_legend_handles_labels()\n",
    "handles.extend(axes[1, 0].get_legend_handles_labels())\n",
    "fig.legend(\n",
    "    handles,\n",
    "    [l.split(\":\")[0] for l in labels_leg],\n",
    "    loc=\"upper right\",\n",
    "    bbox_to_anchor=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# Adjust main plot layout to make room on the right for the colorbar\n",
    "plt.tight_layout(rect=[0, 0, 0.88, 0.96])\n",
    "\n",
    "# Add a new, separate axes for the colorbar\n",
    "cbar_ax = fig.add_axes([0.9, 0.15, 0.03, 0.7])\n",
    "cbar = fig.colorbar(pcm, cax=cbar_ax)\n",
    "cbar.set_label(\"Probability Density\", size=12)\n",
    "\n",
    "# Show all plots at the end\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a7ded",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have successfully performed a Bayesian inversion to infer a global ice melt pattern from sparse, noisy data from the GLOSS tide gauge network.\n",
    "\n",
    "The results show that:\n",
    "* The **posterior expectation** for ice melt successfully recovers the large-scale features of the **true model**, even though it was derived from only a small number of global tide gauges.\n",
    "* The **predicted sea-level fingerprint** from our inversion result is a good match for the true fingerprint, demonstrating that our solution is consistent with the physics of the problem.\n",
    "* The estimates for GMSL change are quite accurate, with our results also providing rigorous uncertainties subject to our choice of prior. \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyslfp-si9MxX7m-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
